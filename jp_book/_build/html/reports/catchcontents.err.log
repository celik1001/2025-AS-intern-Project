Traceback (most recent call last):
  File "/Users/zhangweiqin/Library/Python/3.10/lib/python/site-packages/jupyter_core/utils/__init__.py", line 154, in wrapped
    asyncio.get_running_loop()
RuntimeError: no running event loop

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/zhangweiqin/Library/Python/3.10/lib/python/site-packages/jupyter_cache/executors/utils.py", line 58, in single_nb_execution
    executenb(
  File "/Users/zhangweiqin/Library/Python/3.10/lib/python/site-packages/nbclient/client.py", line 1319, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
  File "/Users/zhangweiqin/Library/Python/3.10/lib/python/site-packages/jupyter_core/utils/__init__.py", line 158, in wrapped
    return loop.run_until_complete(inner)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py", line 641, in run_until_complete
    return future.result()
  File "/Users/zhangweiqin/Library/Python/3.10/lib/python/site-packages/nbclient/client.py", line 709, in async_execute
    await self.async_execute_cell(
  File "/Users/zhangweiqin/Library/Python/3.10/lib/python/site-packages/nbclient/client.py", line 1062, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/Users/zhangweiqin/Library/Python/3.10/lib/python/site-packages/nbclient/client.py", line 918, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
import requests
import json
import pandas as pd
from bs4 import BeautifulSoup
import time
import logging
from datetime import datetime

# ------------------ Logging 設定 ------------------
logging.basicConfig(
    filename='wayback.log',
    filemode='a',
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
)
# ---------------------------------------------------


class WaybackScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def get_latest_snapshot(self, url):
        params = {
            'url': url,
            'output': 'json',
            'filter': 'statuscode:200',
            'sort': 'reverse',
            'limit': 1
        }
        try:
            r = self.session.get("https://web.archive.org/cdx/search/cdx", params=params)
            r.raise_for_status()
            data = r.json()
            if len(data) > 1:
                row = data[1]
                return {
                    'timestamp': row[1],
                    'original_url': row[2],
                    'wayback_url': f"https://web.archive.org/web/{row[1]}/{row[2]}"
                }
        except Exception as e:
            logging.error(f"[CDX錯誤] {url} → {e}")
        return None

    def scrape_webpage_content(self, wayback_url, max_retries=3):
        for attempt in range(1, max_retries + 1):
            try:
                resp = self.session.get(wayback_url, timeout=10)
                resp.raise_for_status()
                soup = BeautifulSoup(resp.content, 'html.parser')
                return {
                    'url': wayback_url,
                    'title': self.safe_extract(soup.find('title')),
                    'h1_tags': [h1.get_text().strip() for h1 in soup.find_all('h1')],
                    'content': "\n".join([p.get_text().strip() for p in soup.find_all('p') if p.get_text().strip()])
                }
            except Exception as e:
                logging.warning(f"[爬取錯誤] ({attempt}) {wayback_url} → {e}")
                time.sleep(2)
        return None

    def safe_extract(self, element):
        return element.get_text().strip() if element else ''

    def format_timestamp(self, ts):
        try:
            return datetime.strptime(ts, "%Y%m%d%H%M%S").strftime("%Y-%m-%d %H:%M:%S")
        except:
            return ts


def main():
    csv_path = "only_in_b.csv"
    url_column_name = "uri"
    N_LIMIT = 50   # None = 全部

    df = pd.read_csv(csv_path)
    urls = df[url_column_name].dropna().tolist()

    if N_LIMIT:
        urls = urls[:N_LIMIT]
        logging.info(f"N_LIMIT 開啟，只處理前 {N_LIMIT} 筆")

    print(f"共讀取 {len(urls)} 個 URL，開始處理…")
    logging.info(f"開始處理 {len(urls)} 個 URL")

    scraper = WaybackScraper()
    all_data = []

    for idx, target_url in enumerate(urls, 1):
        logging.info(f"[{idx}/{len(urls)}] {target_url}")
        latest = scraper.get_latest_snapshot(target_url)
        if latest:
            logging.info(f"  → 最新快照: {latest['wayback_url']}")
            content = scraper.scrape_webpage_content(latest['wayback_url'])
            if content:
                content['target_url'] = target_url
                content['timestamp'] = latest['timestamp']
                content['formatted_date'] = scraper.format_timestamp(latest['timestamp'])
                all_data.append(content)
                logging.info("  [v] 成功")
                print(f"[{idx}] v {target_url}")
            else:
                logging.error("  [x] 內容抓取失敗")
                print(f"[{idx}] x 內容失敗")
        else:
            logging.error("  [x] 沒找到快照")
            print(f"[{idx}] x 無快照")

    # --- 存檔 ---
    if all_data:
        with open("batch_latest_wayback_data.json", "w", encoding="utf-8") as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        logging.info("JSON 已輸出 batch_latest_wayback_data.json")

        simplified = [{
            'target_url': d.get('target_url'),
            'timestamp': d.get('timestamp'),
            'formatted_date': d.get('formatted_date'),
            'title': d.get('title', ''),
            'content': d.get('content', ''),
            'h1_count': len(d.get('h1_tags', [])),
            'wayback_url': d.get('url')
        } for d in all_data]
        pd.DataFrame(simplified).to_csv("batch_latest_wayback_summary.csv", index=False, encoding="utf-8-sig")
        logging.info("CSV 已輸出 batch_latest_wayback_summary.csv")
        print("[v] 已輸出 json / csv")
    else:
        print("全部網址失敗")


if __name__ == "__main__":
    start = time.time()
    start_dt = datetime.now()
    print("開始時間：", start_dt.strftime("%Y-%m-%d %H:%M:%S"))
    logging.info("=====  程式開始  =====")

    main()
    
    end = time.time()
    end_dt = datetime.now()
    print("結束時間：", end_dt.strftime("%Y-%m-%d %H:%M:%S"))
    print(f"總耗時：{end - start:.2f} 秒")
    logging.info(f"===== 程式結束，總耗時 {end - start:.2f} 秒 =====")

------------------

----- stdout -----
開始時間： 2025-08-26 15:05:30
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
Cell [0;32mIn[1], line 142[0m
[1;32m    139[0m [38;5;28mprint[39m([38;5;124m"[39m[38;5;124m開始時間：[39m[38;5;124m"[39m, start_dt[38;5;241m.[39mstrftime([38;5;124m"[39m[38;5;124m%[39m[38;5;124mY-[39m[38;5;124m%[39m[38;5;124mm-[39m[38;5;132;01m%d[39;00m[38;5;124m [39m[38;5;124m%[39m[38;5;124mH:[39m[38;5;124m%[39m[38;5;124mM:[39m[38;5;124m%[39m[38;5;124mS[39m[38;5;124m"[39m))
[1;32m    140[0m logging[38;5;241m.[39minfo([38;5;124m"[39m[38;5;124m=====  程式開始  =====[39m[38;5;124m"[39m)
[0;32m--> 142[0m [43mmain[49m[43m([49m[43m)[49m
[1;32m    144[0m end [38;5;241m=[39m time[38;5;241m.[39mtime()
[1;32m    145[0m end_dt [38;5;241m=[39m datetime[38;5;241m.[39mnow()

Cell [0;32mIn[1], line 81[0m, in [0;36mmain[0;34m()[0m
[1;32m     78[0m url_column_name [38;5;241m=[39m [38;5;124m"[39m[38;5;124muri[39m[38;5;124m"[39m
[1;32m     79[0m N_LIMIT [38;5;241m=[39m [38;5;241m50[39m   [38;5;66;03m# None = 全部[39;00m
[0;32m---> 81[0m df [38;5;241m=[39m [43mpd[49m[38;5;241;43m.[39;49m[43mread_csv[49m[43m([49m[43mcsv_path[49m[43m)[49m
[1;32m     82[0m urls [38;5;241m=[39m df[url_column_name][38;5;241m.[39mdropna()[38;5;241m.[39mtolist()
[1;32m     84[0m [38;5;28;01mif[39;00m N_LIMIT:

File [0;32m~/Library/Python/3.10/lib/python/site-packages/pandas/io/parsers/readers.py:1026[0m, in [0;36mread_csv[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)[0m
[1;32m   1013[0m kwds_defaults [38;5;241m=[39m _refine_defaults_read(
[1;32m   1014[0m     dialect,
[1;32m   1015[0m     delimiter,
[0;32m   (...)[0m
[1;32m   1022[0m     dtype_backend[38;5;241m=[39mdtype_backend,
[1;32m   1023[0m )
[1;32m   1024[0m kwds[38;5;241m.[39mupdate(kwds_defaults)
[0;32m-> 1026[0m [38;5;28;01mreturn[39;00m [43m_read[49m[43m([49m[43mfilepath_or_buffer[49m[43m,[49m[43m [49m[43mkwds[49m[43m)[49m

File [0;32m~/Library/Python/3.10/lib/python/site-packages/pandas/io/parsers/readers.py:620[0m, in [0;36m_read[0;34m(filepath_or_buffer, kwds)[0m
[1;32m    617[0m _validate_names(kwds[38;5;241m.[39mget([38;5;124m"[39m[38;5;124mnames[39m[38;5;124m"[39m, [38;5;28;01mNone[39;00m))
[1;32m    619[0m [38;5;66;03m# Create the parser.[39;00m
[0;32m--> 620[0m parser [38;5;241m=[39m [43mTextFileReader[49m[43m([49m[43mfilepath_or_buffer[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwds[49m[43m)[49m
[1;32m    622[0m [38;5;28;01mif[39;00m chunksize [38;5;129;01mor[39;00m iterator:
[1;32m    623[0m     [38;5;28;01mreturn[39;00m parser

File [0;32m~/Library/Python/3.10/lib/python/site-packages/pandas/io/parsers/readers.py:1620[0m, in [0;36mTextFileReader.__init__[0;34m(self, f, engine, **kwds)[0m
[1;32m   1617[0m     [38;5;28mself[39m[38;5;241m.[39moptions[[38;5;124m"[39m[38;5;124mhas_index_names[39m[38;5;124m"[39m] [38;5;241m=[39m kwds[[38;5;124m"[39m[38;5;124mhas_index_names[39m[38;5;124m"[39m]
[1;32m   1619[0m [38;5;28mself[39m[38;5;241m.[39mhandles: IOHandles [38;5;241m|[39m [38;5;28;01mNone[39;00m [38;5;241m=[39m [38;5;28;01mNone[39;00m
[0;32m-> 1620[0m [38;5;28mself[39m[38;5;241m.[39m_engine [38;5;241m=[39m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_make_engine[49m[43m([49m[43mf[49m[43m,[49m[43m [49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mengine[49m[43m)[49m

File [0;32m~/Library/Python/3.10/lib/python/site-packages/pandas/io/parsers/readers.py:1880[0m, in [0;36mTextFileReader._make_engine[0;34m(self, f, engine)[0m
[1;32m   1878[0m     [38;5;28;01mif[39;00m [38;5;124m"[39m[38;5;124mb[39m[38;5;124m"[39m [38;5;129;01mnot[39;00m [38;5;129;01min[39;00m mode:
[1;32m   1879[0m         mode [38;5;241m+[39m[38;5;241m=[39m [38;5;124m"[39m[38;5;124mb[39m[38;5;124m"[39m
[0;32m-> 1880[0m [38;5;28mself[39m[38;5;241m.[39mhandles [38;5;241m=[39m [43mget_handle[49m[43m([49m
[1;32m   1881[0m [43m    [49m[43mf[49m[43m,[49m
[1;32m   1882[0m [43m    [49m[43mmode[49m[43m,[49m
[1;32m   1883[0m [43m    [49m[43mencoding[49m[38;5;241;43m=[39;49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43moptions[49m[38;5;241;43m.[39;49m[43mget[49m[43m([49m[38;5;124;43m"[39;49m[38;5;124;43mencoding[39;49m[38;5;124;43m"[39;49m[43m,[49m[43m [49m[38;5;28;43;01mNone[39;49;00m[43m)[49m[43m,[49m
[1;32m   1884[0m [43m    [49m[43mcompression[49m[38;5;241;43m=[39;49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43moptions[49m[38;5;241;43m.[39;49m[43mget[49m[43m([49m[38;5;124;43m"[39;49m[38;5;124;43mcompression[39;49m[38;5;124;43m"[39;49m[43m,[49m[43m [49m[38;5;28;43;01mNone[39;49;00m[43m)[49m[43m,[49m
[1;32m   1885[0m [43m    [49m[43mmemory_map[49m[38;5;241;43m=[39;49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43moptions[49m[38;5;241;43m.[39;49m[43mget[49m[43m([49m[38;5;124;43m"[39;49m[38;5;124;43mmemory_map[39;49m[38;5;124;43m"[39;49m[43m,[49m[43m [49m[38;5;28;43;01mFalse[39;49;00m[43m)[49m[43m,[49m
[1;32m   1886[0m [43m    [49m[43mis_text[49m[38;5;241;43m=[39;49m[43mis_text[49m[43m,[49m
[1;32m   1887[0m [43m    [49m[43merrors[49m[38;5;241;43m=[39;49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43moptions[49m[38;5;241;43m.[39;49m[43mget[49m[43m([49m[38;5;124;43m"[39;49m[38;5;124;43mencoding_errors[39;49m[38;5;124;43m"[39;49m[43m,[49m[43m [49m[38;5;124;43m"[39;49m[38;5;124;43mstrict[39;49m[38;5;124;43m"[39;49m[43m)[49m[43m,[49m
[1;32m   1888[0m [43m    [49m[43mstorage_options[49m[38;5;241;43m=[39;49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43moptions[49m[38;5;241;43m.[39;49m[43mget[49m[43m([49m[38;5;124;43m"[39;49m[38;5;124;43mstorage_options[39;49m[38;5;124;43m"[39;49m[43m,[49m[43m [49m[38;5;28;43;01mNone[39;49;00m[43m)[49m[43m,[49m
[1;32m   1889[0m [43m[49m[43m)[49m
[1;32m   1890[0m [38;5;28;01massert[39;00m [38;5;28mself[39m[38;5;241m.[39mhandles [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m
[1;32m   1891[0m f [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mhandles[38;5;241m.[39mhandle

File [0;32m~/Library/Python/3.10/lib/python/site-packages/pandas/io/common.py:873[0m, in [0;36mget_handle[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)[0m
[1;32m    868[0m [38;5;28;01melif[39;00m [38;5;28misinstance[39m(handle, [38;5;28mstr[39m):
[1;32m    869[0m     [38;5;66;03m# Check whether the filename is to be opened in binary mode.[39;00m
[1;32m    870[0m     [38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.[39;00m
[1;32m    871[0m     [38;5;28;01mif[39;00m ioargs[38;5;241m.[39mencoding [38;5;129;01mand[39;00m [38;5;124m"[39m[38;5;124mb[39m[38;5;124m"[39m [38;5;129;01mnot[39;00m [38;5;129;01min[39;00m ioargs[38;5;241m.[39mmode:
[1;32m    872[0m         [38;5;66;03m# Encoding[39;00m
[0;32m--> 873[0m         handle [38;5;241m=[39m [38;5;28;43mopen[39;49m[43m([49m
[1;32m    874[0m [43m            [49m[43mhandle[49m[43m,[49m
[1;32m    875[0m [43m            [49m[43mioargs[49m[38;5;241;43m.[39;49m[43mmode[49m[43m,[49m
[1;32m    876[0m [43m            [49m[43mencoding[49m[38;5;241;43m=[39;49m[43mioargs[49m[38;5;241;43m.[39;49m[43mencoding[49m[43m,[49m
[1;32m    877[0m [43m            [49m[43merrors[49m[38;5;241;43m=[39;49m[43merrors[49m[43m,[49m
[1;32m    878[0m [43m            [49m[43mnewline[49m[38;5;241;43m=[39;49m[38;5;124;43m"[39;49m[38;5;124;43m"[39;49m[43m,[49m
[1;32m    879[0m [43m        [49m[43m)[49m
[1;32m    880[0m     [38;5;28;01melse[39;00m:
[1;32m    881[0m         [38;5;66;03m# Binary mode[39;00m
[1;32m    882[0m         handle [38;5;241m=[39m [38;5;28mopen[39m(handle, ioargs[38;5;241m.[39mmode)

[0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: 'only_in_b.csv'

