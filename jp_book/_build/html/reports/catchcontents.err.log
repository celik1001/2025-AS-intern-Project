Traceback (most recent call last):
  File "/Users/zhangweiqin/Library/Python/3.10/lib/python/site-packages/jupyter_core/utils/__init__.py", line 154, in wrapped
    asyncio.get_running_loop()
RuntimeError: no running event loop

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/zhangweiqin/Library/Python/3.10/lib/python/site-packages/jupyter_cache/executors/utils.py", line 58, in single_nb_execution
    executenb(
  File "/Users/zhangweiqin/Library/Python/3.10/lib/python/site-packages/nbclient/client.py", line 1319, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
  File "/Users/zhangweiqin/Library/Python/3.10/lib/python/site-packages/jupyter_core/utils/__init__.py", line 158, in wrapped
    return loop.run_until_complete(inner)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py", line 641, in run_until_complete
    return future.result()
  File "/Users/zhangweiqin/Library/Python/3.10/lib/python/site-packages/nbclient/client.py", line 709, in async_execute
    await self.async_execute_cell(
  File "/Users/zhangweiqin/Library/Python/3.10/lib/python/site-packages/nbclient/client.py", line 1062, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/Users/zhangweiqin/Library/Python/3.10/lib/python/site-packages/nbclient/client.py", line 918, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
# ä½¿ç”¨ BeautifulSoupã€requests ä¾†æ“·å– Wayback Machine ä¸Šçš„ç¶²é å…§å®¹ä¸¦å­˜æˆ

## ç°¡ä»‹

é€™æ”¯ç¨‹å¼çš„ç›®çš„æ˜¯å”åŠ©ä½¿ç”¨è€…æ“·å–ç‰¹å®šå¹´æœˆã€é¡žåˆ¥çš„æ–‡ç« åˆ—è¡¨ï¼Œ
é€éŽ Wayback CDX API èˆ‡`requests`ã€`BeautifulSoup`å–å¾—æ–‡ç«  urlã€Wayback å¿«å–çš„ç¶²å€ã€æ–‡ç«  idï¼Œèƒ½å¤ åˆæ­¥æŸ¥çœ‹ç¸½å…±æœ‰å“ªäº›æ–‡ç« ã€‚

ç‚ºäº†åŠ å¿«é€Ÿåº¦ï¼Œä½¿ç”¨äº†å¤šåŸ·è¡Œç·’ä¾†åŠ å¿«ç¨‹å¼é‹ä½œã€‚

```{tip}
ä»¥ä¸‹çš†æ˜¯åœ¨Python3ä¸­åŸ·è¡Œã€‚
åœ¨é–‹å§‹ä¹‹å‰ï¼Œå»ºè­°å…ˆé–‹ä¸€å€‹è™›æ“¬ç’°å¢ƒï¼Œé¿å…è¡çªã€‚
```

     python3 -m venv .venv  # å»ºç«‹è™›æ“¬ç’°å¢ƒ (è³‡æ–™å¤¾åç¨±å¯è‡ªè¨‚ï¼Œä¸€èˆ¬ç”¨ .venv æˆ– venv)
    source .venv/bin/activate # å•Ÿå‹•è™›æ“¬ç’°å¢ƒ macOS / Linux
    .venv\Scripts\activate # Windows

```

```

````{note}
é€™æ”¯ç¨‹å¼æœƒä½¿ç”¨ `requests`ã€`Beautiful Soup`é€™å€‹å¥—ä»¶èˆ‡ Wayback Machine åšäº’å‹•ã€‚
åœ¨é–‹å§‹ä¹‹å‰ï¼Œè«‹å…ˆå®‰è£ï¼š
    ```
        pip install requests BeautifulSoup
    ```
````

```python
import os
import time
import json
import zipfile
import requests
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse
```

```python
# settings
INPUT_CSV = "example.csv"  # Input file with article URLs
URL_FIELD = "uri"
ID_FIELD = "id"
THREADS = 5
N_LIMIT = 10                    # None = all rows
OUTPUT_PREFIX = "output_"
SKIP_FILE_NAME = "skipped_urls.txt"
```

```python
# Utility functions
def _only_date(s: str) -> str:
    """
    Normalize different datetime formats into YYYY-MM-DD.
    Supported inputs include:
    - '20240207123456' (CDX timestamp)
    - '2024-02-07T12:34:56+08:00'
    - '2024-02-07 12:34:56'
    - '2024-02-07'
    Returns empty string if parsing fails.
    """
    if not s:
        return ""
    s = s.strip()
    if s.isdigit() and len(s) == 14:  # CDX format
        try:
            return datetime.strptime(s, "%Y%m%d%H%M%S").strftime("%Y-%m-%d")
        except Exception:
            pass
    for fmt in ("%Y-%m-%dT%H:%M:%S%z",
                "%Y-%m-%dT%H:%M:%S",
                "%Y-%m-%d %H:%M:%S",
                "%Y-%m-%d"):
        try:
            dt = datetime.strptime(s.replace("Z", "+0000"), fmt)
            return dt.strftime("%Y-%m-%d")
        except Exception:
            continue
    if len(s) >= 10 and s[4] == "-" and s[7] == "-":
        return s[:10]
    return ""


def _first_path_segment(raw_url: str) -> str:
    """
    Return the first path segment of a URL.
    Example: /local/20200729/... -> 'local'
    """
    try:
        p = urlparse(raw_url)
        parts = [seg for seg in p.path.split("/") if seg]
        return parts[0] if parts else ""
    except Exception:
        return ""


def _ensure_dir(path: str):
    os.makedirs(path, exist_ok=True)
```

```python
# Wayback Scraper
class WaybackScraper:
    def __init__(self, img_root_dir):
        self.session = requests.Session()
        self.session.headers.update({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'})
        self.img_root_dir = img_root_dir
        _ensure_dir(self.img_root_dir)

    def get_latest_snapshot(self, url):
        """
        Query the CDX API for the latest snapshot of a given URL.
        Returns timestamp, original URL, and constructed Wayback URL.
        """
        api = "https://web.archive.org/cdx/search/cdx"
        params = {
            'url': url,
            'output': 'json',
            'filter': 'statuscode:200',
            'sort': 'reverse',
            'limit': 1
        }
        try:
            r = self.session.get(api, params=params, timeout=20)
            r.raise_for_status()
            data = r.json()
            if len(data) > 1:
                row = data[1]
                return {
                    'timestamp': row[1],
                    'original_url': row[2],
                    'wayback_url': f"https://web.archive.org/web/{row[1]}/{row[2]}"
                }
        except Exception as e:
            print(f"[CDX error] {url} â†’ {e}, params={params}")
        return None

    def _extract_published_date(self, soup, wayback_ts: str) -> str:
        """
        Try to extract a published date from <meta property='article:published_time'>
        or <time datetime>. Fall back to Wayback timestamp if none are found.
        """
        meta = soup.find("meta", attrs={"property": "article:published_time"})
        if meta and meta.get("content"):
            d = _only_date(meta.get("content"))
            if d:
                return d
        t = soup.find("time")
        if t:
            d = _only_date(t.get("datetime") or t.get_text(strip=True))
            if d:
                return d
        return _only_date(wayback_ts)

    def scrape_article(self, wayback_url: str, raw_url: str, wayback_ts: str):
        """
        Parse a Wayback snapshot page to extract:
        - title
        - body text (concatenated <p> tags)
        - publication date
        - category (from URL path)
        - image URLs (first = cover, rest = others)
        """
        try:
            resp = self.session.get(wayback_url, timeout=20)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.content, 'html.parser')

            # Title
            title_tag = soup.find("h1") or soup.find("title")
            title = title_tag.get_text(strip=True) if title_tag else ""

            # Body text
            bodies = "\n".join(
                p.get_text().strip()
                for p in soup.find_all("p")
                if p.get_text().strip()
            )

            # Dates
            firstcreated = self._extract_published_date(soup, wayback_ts)
            contentcreated = firstcreated
            versioncreated = _only_date(wayback_ts)

            # Collect images
            all_img_urls = []
            for img in soup.find_all("img"):
                src = img.get("src")
                if not src:
                    continue
                if src.startswith("//"):
                    src = "https:" + src
                elif not src.startswith("http"):
                    src = f"https://web.archive.org{src}"
                if src not in all_img_urls:
                    all_img_urls.append(src)

            subject = _first_path_segment(raw_url)

            return {
                "title": title,
                "bodies": bodies,
                "firstcreated": firstcreated,
                "versioncreated": versioncreated,
                "contentcreated": contentcreated,
                "subject": subject,
                "all_img_urls": all_img_urls
            }
        except Exception as e:
            print(f"[Parse error] {wayback_url} (raw={raw_url}) â†’ {e}")
            return None

    def download_images(self, urls, item_id):
        """
        Download all images for an article.
        Saved to images/{item_id}/img/
        Filenames: {item_id}_cover_1.jpg (first), {item_id}_other_2.jpg (rest).
        Returns a list of filenames aligned with input URLs.
        """
        saved_files = []
        item_img_dir = os.path.join(self.img_root_dir, item_id, "img")
        _ensure_dir(item_img_dir)

        for i, url in enumerate(urls, 1):
            try:
                ext = os.path.splitext(url)[1].split("?")[0].lower()
                if ext not in ['.jpg', '.jpeg', '.png', '.gif', '.webp']:
                    ext = '.jpg'
                label = "cover" if i == 1 else "other"
                fname = f"{item_id}_{label}_{i}{ext}"
                fpath = os.path.join(item_img_dir, fname)

                r = self.session.get(url, timeout=20)
                if r.status_code == 200:
                    with open(fpath, "wb") as f:
                        f.write(r.content)
                    saved_files.append(fname)
                else:
                    print(f"[Download failed {r.status_code}] {url}")
                    saved_files.append("")
            except Exception as e:
                print(f"[Download error] {url} â†’ {e}")
                saved_files.append("")
        return saved_files
```

------------------


[0;36m  File [0;32m<tokenize>:16[0;36m[0m
[0;31m    source .venv/bin/activate # å•Ÿå‹•è™›æ“¬ç’°å¢ƒ macOS / Linux[0m
[0m    ^[0m
[0;31mIndentationError[0m[0;31m:[0m unindent does not match any outer indentation level


