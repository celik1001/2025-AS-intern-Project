Traceback (most recent call last):
  File "/Users/zhangweiqin/Library/Python/3.10/lib/python/site-packages/jupyter_core/utils/__init__.py", line 154, in wrapped
    asyncio.get_running_loop()
RuntimeError: no running event loop

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/zhangweiqin/Library/Python/3.10/lib/python/site-packages/jupyter_cache/executors/utils.py", line 58, in single_nb_execution
    executenb(
  File "/Users/zhangweiqin/Library/Python/3.10/lib/python/site-packages/nbclient/client.py", line 1319, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
  File "/Users/zhangweiqin/Library/Python/3.10/lib/python/site-packages/jupyter_core/utils/__init__.py", line 158, in wrapped
    return loop.run_until_complete(inner)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py", line 641, in run_until_complete
    return future.result()
  File "/Users/zhangweiqin/Library/Python/3.10/lib/python/site-packages/nbclient/client.py", line 709, in async_execute
    await self.async_execute_cell(
  File "/Users/zhangweiqin/Library/Python/3.10/lib/python/site-packages/nbclient/client.py", line 1062, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/Users/zhangweiqin/Library/Python/3.10/lib/python/site-packages/nbclient/client.py", line 918, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
# ä½¿ç”¨ CDX api æŸ¥æ‰¾ æ™‚é–“ç¯„åœå…§çš„ç¶²å€å…§å®¹

## ç°¡ä»‹

é€™æ”¯ç¨‹å¼çš„ç›®çš„æ˜¯å”åŠ©ä½¿ç”¨è€…ç²å¾—ç‰¹å®šå¹´æœˆã€é¡åˆ¥çš„æ–‡ç« åˆ—è¡¨ï¼Œ
é€é Wayback CDX API èˆ‡`requests`å–å¾—æ–‡ç«  urlã€Wayback å¿«å–çš„ç¶²å€ã€æ–‡ç«  idï¼Œèƒ½å¤ åˆæ­¥æŸ¥çœ‹ç¸½å…±æœ‰å“ªäº›æ–‡ç« ã€‚

ç‚ºäº†åŠ å¿«é€Ÿåº¦ï¼Œä½¿ç”¨äº†å¤šåŸ·è¡Œç·’ä¾†åŠ å¿«ç¨‹å¼é‹ä½œã€‚

```{tip}
ä»¥ä¸‹çš†æ˜¯åœ¨Python3ä¸­åŸ·è¡Œã€‚
åœ¨é–‹å§‹ä¹‹å‰ï¼Œå»ºè­°å…ˆé–‹ä¸€å€‹è™›æ“¬ç’°å¢ƒï¼Œé¿å…è¡çªã€‚
```

     python3 -m venv .venv  # å»ºç«‹è™›æ“¬ç’°å¢ƒ (è³‡æ–™å¤¾åç¨±å¯è‡ªè¨‚ï¼Œä¸€èˆ¬ç”¨ .venv æˆ– venv)
    source .venv/bin/activate # å•Ÿå‹•è™›æ“¬ç’°å¢ƒ macOS / Linux
    .venv\Scripts\activate # Windows

```

```

````{note}
é€™æ”¯ç¨‹å¼æœƒä½¿ç”¨ `requests` é€™å€‹å¥—ä»¶èˆ‡ Wayback Machine åšäº’å‹•ã€‚
åœ¨é–‹å§‹ä¹‹å‰ï¼Œè«‹å…ˆå®‰è£ï¼š
    ```
        pip install requests
    ```
````

```python
import requests, csv, time, random
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import logging
import os
from collections import defaultdict
```

```python
# set log
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)
```

```python
API = "https://web.archive.org/cdx/search/cdx"
class MultiCategoryWaybackScraper:
    def __init__(self, max_workers=2, max_retries=3, timeout=60):
        self.max_workers = max_workers
        self.max_retries = max_retries
        self.timeout = timeout
        self.session = requests.Session()
        self.session.headers.update({"Accept-Encoding": "gzip"})
        self.lock = Lock()
        self.stats = defaultdict(lambda: {"success": 0, "failed": 0, "total_items": 0})

    def fetch_with_retry(self, params, info=""):
        """retry request"""
        for attempt in range(self.max_retries):
            try:
                response = self.session.get(API, params=params, timeout=self.timeout)
                response.raise_for_status()
                data = response.json()

                if not data:
                    logger.warning(f"{info} - Empty response on attempt {attempt + 1}")
                    continue

                header, *items = data
                logger.info(f"{info} - Success: {len(items)} items")
                return items

            except requests.exceptions.RequestException as e:
                wait_time = random.uniform(2, 5) * (attempt + 1)
                logger.warning(f"{info} - Attempt {attempt + 1} failed: {e}")

                if attempt < self.max_retries - 1:
                    logger.info(f"{info} - Waiting {wait_time:.1f}s before retry")
                    time.sleep(wait_time)
                else:
                    logger.error(f"{info} - All attempts failed")

            except Exception as e:
                logger.error(f"{info} - Unexpected error: {e}")
                break

        return []

    def fetch_single_day(self, date, category):
        """catch single day"""
        params = {
            "url": f"www.appledaily.com.tw/{category}/{date}/*", # set catagory and date
            "output": "json",
            "fl": "timestamp,original",
            "filter": ["statuscode:200", "mimetype:text/html"],
            "collapse": "original",
            "limit": 5000,
        }

        info = f"[{category}] {date}"
        items = self.fetch_with_retry(params, info)

        with self.lock:
            if items:
                self.stats[category]["success"] += 1
                self.stats[category]["total_items"] += len(items)
            else:
                self.stats[category]["failed"] += 1

        results = []
        for ts, orig in items:
            try:
                clean = orig.split('?', 1)[0]
                article_id = clean.rstrip('/').split('/')[-1]

                if not article_id or article_id == category:
                    continue

                article_url = f"https://www.appledaily.com.tw/{category}/{date}/{article_id}/"
                results.append((date, article_id, article_url, category))
            except Exception as e:
                logger.warning(f"{info} - Error processing item {orig}: {e}")

        return results

    def fetch_category_data(self, months, category):
        """catch all month of category"""
        logger.info(f"Starting category [{category}] for months: {months}")
        category_start = time.time()

        # generate all dates
        all_dates = []
        for month in months:
            for day in range(1, 32):
                all_dates.append(f"{month}{day:02d}")

        all_rows = []
        failed_requests = []

        # multiThreads
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_info = {
                executor.submit(self.fetch_single_day, date, category): (date, category)
                for date in all_dates
            }

            completed = 0
            total = len(future_to_info)

            for future in as_completed(future_to_info):
                date, cat = future_to_info[future]
                completed += 1

                try:
                    results = future.result()
                    if results:
                        all_rows.extend(results)
                    else:
                        failed_requests.append((date, cat))

                    # Progress bar process per 50
                    if completed % 50 == 0:
                        logger.info(f"[{category}] Progress: {completed}/{total} ({completed/total*100:.1f}%)")

                except Exception as e:
                    logger.error(f"[{category}] {date} - Future exception: {e}")
                    failed_requests.append((date, cat))

        # retry failed request
        if failed_requests:
            logger.info(f"[{category}] Retrying {len(failed_requests)} failed requests")
            retry_count = 0
            for date, cat in failed_requests[:]:  # copy failed lists
                time.sleep(random.uniform(3, 6))  # set retry timeout
                retry_results = self.fetch_single_day(date, cat)
                if retry_results:
                    all_rows.extend(retry_results)
                    failed_requests.remove((date, cat))
                    retry_count += 1

                # Retry progress
                if retry_count % 10 == 0:
                    logger.info(f"[{category}] Retry progress: {retry_count}")

        category_elapsed = time.time() - category_start
        logger.info(f"[{category}] Completed in {category_elapsed:.2f}s, "
                   f"got {len(all_rows)} articles, {len(failed_requests)} failed")

        return all_rows, failed_requests

    def save_category_to_csv(self, rows, category, months):
        """saved sing. category"""
        if not rows:
            logger.warning(f"[{category}] No data to save")
            return 0

        # unique articles
        unique_articles = {}
        for date, article_id, url, cat in rows:
            key = f"{cat}_{article_id}"
            if key not in unique_articles:
                unique_articles[key] = (date, article_id, url, cat)
            else:
                existing_date = unique_articles[key][0]
                if date < existing_date:
                    unique_articles[key] = (date, article_id, url, cat)

        # sorted and write into file
        sorted_articles = sorted(unique_articles.values())

        month_range = f"{months[0]}-{months[-1]}"
        filename = f"appledaily_{category}_{month_range}.csv"

        with open(filename, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(["date", "article_id", "url", "category"])
            for row in sorted_articles:
                writer.writerow(row)

        logger.info(f"[{category}] Saved {len(sorted_articles)} unique articles to {filename}")
        return len(sorted_articles), filename

    def save_all_categories_to_csv(self, all_data, months):
        """save all categories to csv"""
        if not all_data:
            logger.warning("No data to save to combined file")
            return 0, ""

        # combine all data and output to csv
        unique_articles = {}
        for rows in all_data.values():
            for date, article_id, url, category in rows:
                key = f"{category}_{article_id}"
                if key not in unique_articles:
                    unique_articles[key] = (date, article_id, url, category)
                else:
                    existing_date = unique_articles[key][0]
                    if date < existing_date:
                        unique_articles[key] = (date, article_id, url, category)

        # sorted and write into file
        sorted_articles = sorted(unique_articles.values())

        month_range = f"{months[0]}-{months[-1]}"
        filename = f"appledaily_ALL_CATEGORIES_{month_range}.csv"

        with open(filename, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(["date", "article_id", "url", "category"])
            for row in sorted_articles:
                writer.writerow(row)

        logger.info(f"Saved {len(sorted_articles)} unique articles (all categories) to {filename}")
        return len(sorted_articles), filename
```

```python
def main():
    # === set variables ===
    months = ["202201", "202202", "202203", "202204", "202205"]
    categories = [
        "local",        # åœ°æ–¹æ–°è
        "realtime",     # å³æ™‚æ–°è
        "entertainment", # å¨›æ¨‚æ–°è
        "sports",       # é«”è‚²æ–°è
        "international", # åœ‹éš›æ–°è
        "finance",      # è²¡ç¶“æ–°è
        "life",         # ç”Ÿæ´»æ–°è
        "forum"         # è«–å£‡
    ]

    # scraper
    scraper = MultiCategoryWaybackScraper(
        max_workers=2,      # 2workers
        max_retries=3,      # retry times
        timeout=60          # timeout:60s
    )

    # start scrapping
    overall_start = time.time()
    all_category_data = {}
    category_files = []

    logger.info("=" * 80)
    logger.info(f"STARTING MULTI-CATEGORY SCRAPING")
    logger.info(f"Months: {months}")
    logger.info(f"Categories: {categories}")
    logger.info("=" * 80)

    for i, category in enumerate(categories, 1):
        logger.info(f"\n{'='*20} CATEGORY {i}/{len(categories)}: {category.upper()} {'='*20}")

        category_rows, failed = scraper.fetch_category_data(months, category)
        all_category_data[category] = category_rows

        # saved sing. category
        if category_rows:
            count, filename = scraper.save_category_to_csv(category_rows, category, months)
            category_files.append((category, count, filename))
        else:
            logger.warning(f"[{category}] No data found!")
            category_files.append((category, 0, "No file"))

        # sleep between both category
        if i < len(categories):
            rest_time = random.uniform(30, 60)
            logger.info(f"Resting {rest_time:.1f}s before next category...")
            time.sleep(rest_time)

    # saved all news
    total_count, combined_file = scraper.save_all_categories_to_csv(all_category_data, months)

    # final report
    total_time = time.time() - overall_start

    logger.info("\n" + "=" * 80)
    logger.info("FINAL REPORT")
    logger.info("=" * 80)
    logger.info(f"Total execution time: {total_time:.2f} seconds ({total_time/60:.1f} minutes)")
    logger.info(f"Months processed: {', '.join(months)}")
    logger.info(f"Categories processed: {len(categories)}")

    logger.info("\nPer-category results:")
    total_articles = 0
    for category, count, filename in category_files:
        logger.info(f"  {category:>15}: {count:>6} articles â†’ {filename}")
        total_articles += count

    logger.info(f"\nCombined results:")
    logger.info(f"  Total raw articles: {total_articles}")
    logger.info(f"  Unique articles: {total_count}")
    logger.info(f"  Combined file: {combined_file}")

    logger.info("\nSuccess rates by category:")
    for category in categories:
        stats = scraper.stats[category]
        if stats["success"] + stats["failed"] > 0:
            success_rate = stats["success"] / (stats["success"] + stats["failed"]) * 100
            logger.info(f"  {category:>15}: {success_rate:>5.1f}% "
                       f"({stats['success']}/{stats['success'] + stats['failed']} requests, "
                       f"{stats['total_items']} items)")
        else:
            logger.info(f"  {category:>15}: No requests made")

    # create SUMMARY file
    summary_file = f"SUMMARY_appledaily_{months[0]}-{months[-1]}.txt"
    with open(summary_file, "w", encoding="utf-8") as f:
        f.write(f"Apple Daily Archive Summary\n")
        f.write(f"Period: {months[0]} - {months[-1]}\n")
        f.write(f"Execution time: {total_time:.2f} seconds\n")
        f.write(f"Total unique articles: {total_count}\n\n")
        f.write("Files created:\n")
        for category, count, filename in category_files:
            if filename != "No file":
                f.write(f"  {filename}\n")
        f.write(f"  {combined_file}\n")

    logger.info(f"\nSummary saved to: {summary_file}")
    logger.info("SCRAPING COMPLETED!")
```

```python
if __name__ == "__main__":
    main()
```

------------------


[0;36m  File [0;32m<tokenize>:16[0;36m[0m
[0;31m    source .venv/bin/activate # å•Ÿå‹•è™›æ“¬ç’°å¢ƒ macOS / Linux[0m
[0m    ^[0m
[0;31mIndentationError[0m[0;31m:[0m unindent does not match any outer indentation level


