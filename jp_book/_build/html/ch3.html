
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>使用 BeautifulSoup、requests 來擷取 Wayback Machine 上的網頁內容並存檔 &#8212; Web Archive 編目的研究與實作：以 Wayback CDX API 建立《蘋果新聞網》索引</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'ch3';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="使用 CDX api 查找 時間範圍內的網址內容" href="ch2.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Web Archive 編目的研究與實作：以 Wayback CDX API 建立《蘋果新聞網》索引 - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Web Archive 編目的研究與實作：以 Wayback CDX API 建立《蘋果新聞網》索引 - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    歡迎！
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="summary.html">專題題目</a></li>
<li class="toctree-l1"><a class="reference internal" href="ch0.html">前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="ch1.html">研究問題、方法及流程</a></li>
<li class="toctree-l1"><a class="reference internal" href="ch2.html">使用 CDX api 查找 時間範圍內的網址內容</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">使用 BeautifulSoup、requests 來擷取 Wayback Machine 上的網頁內容並存檔</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/celik1001/2025-AS-intern-Project.git" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/celik1001/2025-AS-intern-Project.git/issues/new?title=Issue%20on%20page%20%2Fch3.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/ch3.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>使用 BeautifulSoup、requests 來擷取 Wayback Machine 上的網頁內容並存檔</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">簡介</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="beautifulsouprequests-wayback-machine">
<h1>使用 BeautifulSoup、requests 來擷取 Wayback Machine 上的網頁內容並存檔<a class="headerlink" href="#beautifulsouprequests-wayback-machine" title="Link to this heading">#</a></h1>
<section id="id1">
<h2>簡介<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>這支程式的目的是協助使用者擷取特定年月、類別的文章列表，
透過 Wayback CDX API 與<code class="docutils literal notranslate"><span class="pre">requests</span></code>、<code class="docutils literal notranslate"><span class="pre">BeautifulSoup</span></code>取得文章 url、Wayback 快取的網址、文章 id，能夠初步查看總共有哪些文章。</p>
<p>為了加快速度，使用了多執行緒來加快程式運作。</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>以下皆是在Python3中執行。
在開始之前，建議先開一個虛擬環境，避免衝突。</p>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>python3 -m venv .venv  # 建立虛擬環境 (資料夾名稱可自訂，一般用 .venv 或 venv)
source .venv/bin/activate # 啟動虛擬環境 macOS / Linux
.venv\Scripts\activate # Windows
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>```

````{note}
這支程式會使用 `requests`、`Beautiful Soup`這個套件與 Wayback Machine 做互動。
在開始之前，請先安裝：
    ````
        pip install requests BeautifulSoup
    ````
````

```python
import os
import re
import time
import json
import zipfile
import requests
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse
```

```python
# basic setting
INPUT_CSV = &quot;example.csv&quot;
URL_FIELD = &quot;uri&quot;
ID_FIELD = &quot;id&quot;
THREADS = 5
N_LIMIT = 5                  # None == all
OUTPUT_PREFIX = &quot;output_&quot;
SKIP_FILE_NAME = &quot;skipped_urls.txt&quot;
```

```python
# tools
def _only_date(s: str) -&gt; str:
    &quot;&quot;&quot;Normalize time string to YYYY-MM-DD. Return &#39;&#39; if parsing fails.&quot;&quot;&quot;
    if not s:
        return &quot;&quot;
    s = s.strip()
    if s.isdigit() and len(s) == 14:  # CDX: YYYYmmddHHMMSS
        try:
            return datetime.strptime(s, &quot;%Y%m%d%H%M%S&quot;).strftime(&quot;%Y-%m-%d&quot;)
        except Exception:
            pass
    for fmt in (&quot;%Y-%m-%dT%H:%M:%S%z&quot;,
                &quot;%Y-%m-%dT%H:%M:%S&quot;,
                &quot;%Y-%m-%d %H:%M:%S&quot;,
                &quot;%Y-%m-%d&quot;):
        try:
            dt = datetime.strptime(s.replace(&quot;Z&quot;, &quot;+0000&quot;), fmt)
            return dt.strftime(&quot;%Y-%m-%d&quot;)
        except Exception:
            continue
    if len(s) &gt;= 10 and s[4] == &quot;-&quot; and s[7] == &quot;-&quot;:
        return s[:10]
    return &quot;&quot;

def _first_path_segment(raw_url: str) -&gt; str:
    &quot;&quot;&quot;Return first URL path segment (e.g., /local/20200729/... -&gt; local).&quot;&quot;&quot;
    try:
        p = urlparse(raw_url)
        parts = [seg for seg in p.path.split(&quot;/&quot;) if seg]
        return parts[0] if parts else &quot;&quot;
    except Exception:
        return &quot;&quot;

def _norm_img_url(src: str) -&gt; str:
    &quot;&quot;&quot;Normalize to absolute Wayback URL.&quot;&quot;&quot;
    if not src:
        return &quot;&quot;
    src = src.strip()
    if src.startswith(&quot;//&quot;):
        return &quot;https:&quot; + src
    if src.startswith(&quot;/&quot;):
        return &quot;https://web.archive.org&quot; + src
    if not src.startswith(&quot;http&quot;):
        return &quot;https://web.archive.org&quot; + (&quot;/&quot; + src if not src.startswith(&quot;/&quot;) else src)
    return src

def _content_type_from_ext(filename: str) -&gt; str:
    &quot;&quot;&quot;Map file extension to MIME type.&quot;&quot;&quot;
    ext = os.path.splitext(filename)[1].lower().lstrip(&quot;.&quot;)
    if ext in (&quot;jpg&quot;, &quot;jpeg&quot;):
        return &quot;image/jpeg&quot;
    if ext in (&quot;png&quot;, &quot;gif&quot;, &quot;webp&quot;, &quot;bmp&quot;, &quot;svg&quot;):
        return f&quot;image/{ext}&quot;
    return &quot;image/jpeg&quot;

def _normalize_text(s: str) -&gt; str:
    &quot;&quot;&quot;Light normalization for pattern matching (unify slashes, remove extra spaces).&quot;&quot;&quot;
    if not s:
        return &quot;&quot;
    s = s.replace(&quot;／&quot;, &quot;/&quot;).replace(&quot;　&quot;, &quot; &quot;).strip()
    s = re.sub(r&quot;\s+&quot;, &quot; &quot;, s)
    return s

def _extract_by_loc_text(text: str) -&gt; tuple[str, str]:
    &quot;&quot;&quot;
    Extract &#39;by&#39; (reporter) and &#39;located&#39; (place/desk) from text such as:
      - 記者周庭慶／台中報導
      - 地方中心周庭慶／台中報導
      - 周庭慶／台中報導
      - （…）變體皆可；允許 / 與 ／；報導/報道 都接受
    Returns (by, located) or (&quot;&quot;,&quot;&quot;) if not found.
    &quot;&quot;&quot;
    t = _normalize_text(text)

    # Common patterns
    patterns = [
        # Optional prefix + NAME / PLACE 報導|報道
        r&#39;(?:記者|地方中心|採訪中心|特派|特約)?\s*([\u4e00-\u9fa5A-Za-z·．\.\s]{2,20})\s*/\s*([\u4e00-\u9fa5A-Za-z·\.\s]{1,10})\s*報[導道]&#39;,
        # NAME / PLACE 報導|報道
        r&#39;([\u4e00-\u9fa5A-Za-z·．\.\s]{2,20})\s*/\s*([\u4e00-\u9fa5A-Za-z·\.\s]{1,10})\s*報[導道]&#39;,
    ]
    for pat in patterns:
        m = re.search(pat, t)
        if m:
            by = m.group(1).strip(&quot; ，,。.!?（）()&quot;)
            located = m.group(2).strip(&quot; ，,。.!?（）()&quot;)
            return by, located

    # Fallback: only &quot;(PLACE報導)&quot; without a clear name
    m = re.search(r&#39;([\u4e00-\u9fa5A-Za-z·\.\s]{1,10})\s*報[導道]&#39;, t)
    if m:
        return &quot;&quot;, m.group(1).strip(&quot; ，,。.!?（）()&quot;)

    return &quot;&quot;, &quot;&quot;

```

```python
# Wayback Scraper
class WaybackScraper:
    def __init__(self, img_root_dir):
        self.session = requests.Session()
        self.session.headers.update({&#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64)&#39;})
        self.img_root_dir = img_root_dir
        os.makedirs(self.img_root_dir, exist_ok=True)

    def get_latest_snapshot(self, url):
        &quot;&quot;&quot;Query CDX for the latest successful snapshot.&quot;&quot;&quot;
        api = &quot;https://web.archive.org/cdx/search/cdx&quot;
        params = {
            &#39;url&#39;: url,
            &#39;output&#39;: &#39;json&#39;,
            &#39;filter&#39;: &#39;statuscode:200&#39;,
            &#39;sort&#39;: &#39;reverse&#39;,
            &#39;limit&#39;: 1
        }
        try:
            r = self.session.get(api, params=params, timeout=20)
            r.raise_for_status()
            data = r.json()
            if len(data) &gt; 1:
                row = data[1]
                return {
                    &#39;timestamp&#39;: row[1],  # YYYYmmddHHMMSS
                    &#39;original_url&#39;: row[2],
                    &#39;wayback_url&#39;: f&quot;https://web.archive.org/web/{row[1]}/{row[2]}&quot;
                }
        except Exception as e:
            print(f&quot;[CDX error] {url} → {e}&quot;)
        return None

    def _extract_published_date(self, soup, wayback_ts: str) -&gt; str:
        &quot;&quot;&quot;Use meta[article:published_time] or &lt;time&gt;; fallback to Wayback timestamp.&quot;&quot;&quot;
        meta = soup.find(&quot;meta&quot;, attrs={&quot;property&quot;: &quot;article:published_time&quot;})
        if meta and meta.get(&quot;content&quot;):
            d = _only_date(meta.get(&quot;content&quot;))
            if d:
                return d
        t = soup.find(&quot;time&quot;)
        if t:
            d = _only_date(t.get(&quot;datetime&quot;) or t.get_text(strip=True))
            if d:
                return d
        return _only_date(wayback_ts)

    def scrape_article_payload(self, wayback_url, raw_url, item_id, wayback_ts):
        &quot;&quot;&quot;
        Parse snapshot:
          - title
          - body text (joined &lt;p&gt;)
          - published date
          - subject (first path segment)
          - images: FIRST &lt;img&gt; = cover, remaining = other
          - image alts map
          - by / located extraction (author/byline zones → fallback to title+first body chunk)
        &quot;&quot;&quot;
        try:
            resp = self.session.get(wayback_url, timeout=20)
            resp.raise_for_status()
            # prefer lxml; fallback to html.parser
            parser = &quot;lxml&quot;
            try:
                import lxml  # noqa
            except Exception:
                parser = &quot;html.parser&quot;
            soup = BeautifulSoup(resp.content, parser)

            # title
            h1 = soup.find(&quot;h1&quot;) or soup.find(&quot;title&quot;)
            title = h1.get_text(strip=True) if h1 else &quot;&quot;

            # bodies
            paragraphs = [p.get_text().strip() for p in soup.find_all(&quot;p&quot;) if p.get_text().strip()]
            body_text = &quot;\n&quot;.join(paragraphs)

            # date
            published_date = self._extract_published_date(soup, wayback_ts)

            # collect all &lt;img&gt; in order; first is cover, rest are other
            all_imgs, seen = [], set()
            for img in soup.find_all(&quot;img&quot;):
                src = _norm_img_url(img.get(&quot;src&quot;))
                if not src or src in seen:
                    continue
                if &quot;.&quot; not in os.path.basename(src):  # crude filter: must look like a file
                    continue
                all_imgs.append(src)
                seen.add(src)
            cover_urls = all_imgs[:1]
            other_urls = all_imgs[1:]

            # alts
            img_alts = {}
            for img in soup.find_all(&quot;img&quot;):
                src = _norm_img_url(img.get(&quot;src&quot;))
                if not src:
                    continue
                img_alts[src] = (img.get(&quot;alt&quot;) or &quot;&quot;).strip()

            # by/located: search in byline zones first
            meta_zone_texts = []
            for sel in [
                {&quot;name&quot;: &quot;span&quot;, &quot;class_&quot;: re.compile(&quot;author|byline&quot;)},
                {&quot;name&quot;: &quot;div&quot;,  &quot;class_&quot;: re.compile(&quot;author|byline&quot;)},
                {&quot;name&quot;: &quot;p&quot;,    &quot;class_&quot;: re.compile(&quot;author|byline&quot;)},
            ]:
                for el in soup.find_all(sel[&quot;name&quot;], class_=sel[&quot;class_&quot;]):
                    meta_zone_texts.append(el.get_text(&quot; &quot;, strip=True))
            by, located = _extract_by_loc_text(&quot;  &quot;.join(meta_zone_texts))

            # fallback: try title + first body chunk if still empty
            if not by and not located:
                head_and_lead = (title + &quot; &quot; + &quot; &quot;.join(paragraphs[:3]))[:800]
                by, located = _extract_by_loc_text(head_and_lead)

            subject_name = _first_path_segment(raw_url)

            return {
                &quot;title&quot;: title,
                &quot;body_text&quot;: body_text,
                &quot;published_date&quot;: published_date,
                &quot;cover_urls&quot;: cover_urls,
                &quot;other_urls&quot;: other_urls,
                &quot;img_alts&quot;: img_alts,
                &quot;subject_name&quot;: subject_name,
                &quot;by&quot;: by,
                &quot;located&quot;: located,
            }
        except Exception as e:
            print(f&quot;[failed capture] {wayback_url} → {e}&quot;)
            return None

    def download_images(self, urls, item_id, label, start_at=1):
        &quot;&quot;&quot;
        Download images to images/{item_id}/img/
        Filename format: {item_id}_{label}_{N}.{ext}
          - cover uses start_at=1  → {item_id}_cover_1.png
          - other can start at 2   → {item_id}_other_2.png (if a cover exists)
        Returns: list of (url, filename)
        &quot;&quot;&quot;
        saved = []
        item_img_dir = os.path.join(self.img_root_dir, item_id, &quot;img&quot;)
        os.makedirs(item_img_dir, exist_ok=True)

        for i, url in enumerate(urls, 1):
            try:
                ext = os.path.splitext(url)[1].split(&quot;?&quot;)[0].lower()
                if ext not in [&quot;.jpg&quot;, &quot;.jpeg&quot;, &quot;.png&quot;, &quot;.gif&quot;, &quot;.webp&quot;, &quot;.bmp&quot;, &quot;.svg&quot;]:
                    ext = &quot;.jpg&quot;
                seq = start_at + i - 1
                fname = f&quot;{item_id}_{label}_{seq}{ext}&quot;
                fpath = os.path.join(item_img_dir, fname)

                r = self.session.get(url, timeout=20)
                if r.status_code == 200:
                    with open(fpath, &quot;wb&quot;) as f:
                        f.write(r.content)
                    saved.append((url, fname))
                else:
                    print(f&quot;[Download {r.status_code}] {url}&quot;)
            except Exception as e:
                print(f&quot;[Download error] {url} → {e}&quot;)
        return saved
```

```python
# per-row
def process_row(row, scraper: WaybackScraper, skip_file_path: str):
    raw_url = row[URL_FIELD]
    item_id = str(row[ID_FIELD])

    snap = scraper.get_latest_snapshot(raw_url)
    if not snap:
        with open(skip_file_path, &quot;a&quot;, encoding=&quot;utf-8&quot;) as f:
            f.write(raw_url + &quot;\n&quot;)
        return None

    parsed = scraper.scrape_article_payload(
        wayback_url=snap[&#39;wayback_url&#39;],
        raw_url=raw_url,
        item_id=item_id,
        wayback_ts=snap[&#39;timestamp&#39;]
    )
    if not parsed:
        with open(skip_file_path, &quot;a&quot;, encoding=&quot;utf-8&quot;) as f:
            f.write(raw_url + &quot;\n&quot;)
        return None

    # download images with desired numbering
    cover_saved = scraper.download_images(parsed[&quot;cover_urls&quot;], item_id, &quot;cover&quot;, start_at=1)
    other_start = 2 if cover_saved else 1
    other_saved = scraper.download_images(parsed[&quot;other_urls&quot;], item_id, &quot;other&quot;, start_at=other_start)

    # build associations: first = &quot;cover&quot;; others = &quot;other_1&quot;, &quot;other_2&quot;, ...
    associations = []

    def _mk_assoc(name_label, url_fname_tuple):
        url, fname = url_fname_tuple
        href = f&quot;./images/{item_id}/img/{fname}&quot;
        ctype = _content_type_from_ext(fname)
        return {
            &quot;name&quot;: name_label,
            &quot;uri&quot;: url,
            &quot;type&quot;: &quot;picture&quot;,
            &quot;headlines&quot;: [{&quot;value&quot;: parsed[&quot;img_alts&quot;].get(url, &quot;&quot;)}],
            &quot;renditions&quot;: [{&quot;href&quot;: href, &quot;contentType&quot;: ctype}]
        }

    if cover_saved:
        associations.append(_mk_assoc(&quot;cover&quot;, cover_saved[0]))
    for i, tup in enumerate(other_saved, 1):
        associations.append(_mk_assoc(f&quot;other_{i}&quot;, tup))

    # dates
    firstcreated = parsed[&quot;published_date&quot;] or _only_date(snap[&#39;timestamp&#39;])
    versioncreated = _only_date(snap[&#39;timestamp&#39;])  # Wayback snapshot date
    contentcreated = firstcreated
    subjects = [{&quot;name&quot;: parsed[&quot;subject_name&quot;]}] if parsed[&quot;subject_name&quot;] else []

    ninjs_obj = {
        &quot;uri&quot;: raw_url,
        &quot;standard&quot;: {
            &quot;name&quot;: &quot;ninjs&quot;,
            &quot;version&quot;: &quot;3.0&quot;,
            &quot;schema&quot;: &quot;https://www.iptc.org/std/ninjs/ninjs-schema_3.0.json&quot;
        },
        &quot;firstcreated&quot;: firstcreated,
        &quot;versioncreated&quot;: versioncreated,
        &quot;contentcreated&quot;: contentcreated,
        &quot;type&quot;: &quot;text&quot;,
        &quot;language&quot;: &quot;zh-Hant-TW&quot;,
        &quot;headlines&quot;: [{&quot;role&quot;: &quot;main&quot;, &quot;value&quot;: parsed[&quot;title&quot;]}],
        &quot;subjects&quot;: subjects,
        &quot;bodies&quot;: [{&quot;role&quot;: &quot;main&quot;, &quot;contentType&quot;: &quot;text/plain&quot;, &quot;value&quot;: parsed[&quot;body_text&quot;]}],
        &quot;associations&quot;: associations,
        &quot;by&quot;: parsed.get(&quot;by&quot;, &quot;&quot;),          # &lt;-- filled from extractor
        &quot;located&quot;: parsed.get(&quot;located&quot;, &quot;&quot;),# &lt;-- filled from extractor
        &quot;altids&quot;: [{&quot;role&quot;: &quot;internal&quot;, &quot;value&quot;: item_id}]
    }
    return ninjs_obj
```

```python
#  main
if __name__ == &quot;__main__&quot;:
    t0 = time.time()
    print(&quot;activetime:&quot;, datetime.now().strftime(&quot;%Y-%m-%d %H:%M:%S&quot;))

    ts = datetime.now().strftime(&quot;%Y%m%d_%H%M%S&quot;)
    out_dir = f&quot;{OUTPUT_PREFIX}{ts}&quot;
    os.makedirs(out_dir, exist_ok=True)

    img_root_dir = os.path.join(out_dir, &quot;images&quot;)
    skip_file_path = os.path.join(out_dir, SKIP_FILE_NAME)
    os.makedirs(img_root_dir, exist_ok=True)
    if os.path.exists(skip_file_path):
        os.remove(skip_file_path)

    df = pd.read_csv(INPUT_CSV)
    if N_LIMIT:
        df = df.head(N_LIMIT)

    print(&quot;output_index:&quot;, out_dir)
    scraper = WaybackScraper(img_root_dir=img_root_dir)

    ninjs_list = []
    with ThreadPoolExecutor(max_workers=THREADS) as executor:
        tasks = {executor.submit(process_row, row, scraper, skip_file_path): row for _, row in df.iterrows()}
        for i, f in enumerate(as_completed(tasks), 1):
            obj = f.result()
            if obj:
                ninjs_list.append(obj)
                print(f&quot;[{i}] ✔ {obj[&#39;uri&#39;]}&quot;)
            else:
                print(f&quot;[{i}] ✘ skipped&quot;)

    # ninjs.json（array）
    if ninjs_list:
        with open(os.path.join(out_dir, &quot;ninjs.json&quot;), &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
            json.dump(ninjs_list, f, ensure_ascii=False, indent=2)
        print(&quot;already output ninjs.json&quot;)
    else:
        print(&quot;failed&quot;)

    # zip all files
    zip_name = out_dir + &quot;.zip&quot;
    with zipfile.ZipFile(zip_name, &#39;w&#39;, zipfile.ZIP_DEFLATED) as z:
        for root, _, files in os.walk(out_dir):
            for file in files:
                fp = os.path.join(root, file)
                z.write(fp, os.path.relpath(fp, start=os.path.dirname(out_dir)))
    print(f&quot;zipped: {zip_name}&quot;)

    # skip count
    skip_count = 0
    if os.path.exists(skip_file_path):
        with open(skip_file_path, encoding=&quot;utf-8&quot;) as f:
            skip_count = len([l for l in f if l.strip()])
    print(f&quot;total {skip_count} URL skipped&quot;)
    print(&quot;cost: %.2f s&quot; % (time.time() - t0))

```
</pre></div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "celik1001/2025-AS-intern-Project.git",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="ch2.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">使用 CDX api 查找 時間範圍內的網址內容</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">簡介</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Celik1001
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>