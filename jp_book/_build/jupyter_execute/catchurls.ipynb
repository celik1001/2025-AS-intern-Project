{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b2c30d2-73af-4b5e-b83f-ca05196a13dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:16\u001b[0;36m\u001b[0m\n\u001b[0;31m    source .venv/bin/activate # 啟動虛擬環境 macOS / Linux\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# 使用 CDX api 查找 時間範圍內的網址內容\n",
    "\n",
    "## 簡介\n",
    "\n",
    "這支程式的目的是協助使用者獲得特定年月、類別的文章列表，\n",
    "透過 Wayback CDX API 與`requests`取得文章 url、Wayback 快取的網址、文章 id，能夠初步查看總共有哪些文章。\n",
    "\n",
    "為了加快速度，使用了多執行緒來加快程式運作。\n",
    "\n",
    "```{tip}\n",
    "以下皆是在Python3中執行。\n",
    "在開始之前，建議先開一個虛擬環境，避免衝突。\n",
    "```\n",
    "\n",
    "     python3 -m venv .venv  # 建立虛擬環境 (資料夾名稱可自訂，一般用 .venv 或 venv)\n",
    "    source .venv/bin/activate # 啟動虛擬環境 macOS / Linux\n",
    "    .venv\\Scripts\\activate # Windows\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "````{note}\n",
    "這支程式會使用 `requests` 這個套件與 Wayback Machine 做互動。\n",
    "在開始之前，請先安裝：\n",
    "    ```\n",
    "        pip install requests\n",
    "    ```\n",
    "````\n",
    "\n",
    "```python\n",
    "import requests, csv, time, random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "import logging\n",
    "import os\n",
    "from collections import defaultdict\n",
    "```\n",
    "\n",
    "```python\n",
    "# set log\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "```\n",
    "\n",
    "```python\n",
    "API = \"https://web.archive.org/cdx/search/cdx\"\n",
    "class MultiCategoryWaybackScraper:\n",
    "    def __init__(self, max_workers=2, max_retries=3, timeout=60):\n",
    "        self.max_workers = max_workers\n",
    "        self.max_retries = max_retries\n",
    "        self.timeout = timeout\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\"Accept-Encoding\": \"gzip\"})\n",
    "        self.lock = Lock()\n",
    "        self.stats = defaultdict(lambda: {\"success\": 0, \"failed\": 0, \"total_items\": 0})\n",
    "\n",
    "    def fetch_with_retry(self, params, info=\"\"):\n",
    "        \"\"\"retry request\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                response = self.session.get(API, params=params, timeout=self.timeout)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "\n",
    "                if not data:\n",
    "                    logger.warning(f\"{info} - Empty response on attempt {attempt + 1}\")\n",
    "                    continue\n",
    "\n",
    "                header, *items = data\n",
    "                logger.info(f\"{info} - Success: {len(items)} items\")\n",
    "                return items\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                wait_time = random.uniform(2, 5) * (attempt + 1)\n",
    "                logger.warning(f\"{info} - Attempt {attempt + 1} failed: {e}\")\n",
    "\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    logger.info(f\"{info} - Waiting {wait_time:.1f}s before retry\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    logger.error(f\"{info} - All attempts failed\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"{info} - Unexpected error: {e}\")\n",
    "                break\n",
    "\n",
    "        return []\n",
    "\n",
    "    def fetch_single_day(self, date, category):\n",
    "        \"\"\"catch single day\"\"\"\n",
    "        params = {\n",
    "            \"url\": f\"www.appledaily.com.tw/{category}/{date}/*\", # set catagory and date\n",
    "            \"output\": \"json\",\n",
    "            \"fl\": \"timestamp,original\",\n",
    "            \"filter\": [\"statuscode:200\", \"mimetype:text/html\"],\n",
    "            \"collapse\": \"original\",\n",
    "            \"limit\": 5000,\n",
    "        }\n",
    "\n",
    "        info = f\"[{category}] {date}\"\n",
    "        items = self.fetch_with_retry(params, info)\n",
    "\n",
    "        with self.lock:\n",
    "            if items:\n",
    "                self.stats[category][\"success\"] += 1\n",
    "                self.stats[category][\"total_items\"] += len(items)\n",
    "            else:\n",
    "                self.stats[category][\"failed\"] += 1\n",
    "\n",
    "        results = []\n",
    "        for ts, orig in items:\n",
    "            try:\n",
    "                clean = orig.split('?', 1)[0]\n",
    "                article_id = clean.rstrip('/').split('/')[-1]\n",
    "\n",
    "                if not article_id or article_id == category:\n",
    "                    continue\n",
    "\n",
    "                article_url = f\"https://www.appledaily.com.tw/{category}/{date}/{article_id}/\"\n",
    "                results.append((date, article_id, article_url, category))\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"{info} - Error processing item {orig}: {e}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def fetch_category_data(self, months, category):\n",
    "        \"\"\"catch all month of category\"\"\"\n",
    "        logger.info(f\"Starting category [{category}] for months: {months}\")\n",
    "        category_start = time.time()\n",
    "\n",
    "        # generate all dates\n",
    "        all_dates = []\n",
    "        for month in months:\n",
    "            for day in range(1, 32):\n",
    "                all_dates.append(f\"{month}{day:02d}\")\n",
    "\n",
    "        all_rows = []\n",
    "        failed_requests = []\n",
    "\n",
    "        # multiThreads\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            future_to_info = {\n",
    "                executor.submit(self.fetch_single_day, date, category): (date, category)\n",
    "                for date in all_dates\n",
    "            }\n",
    "\n",
    "            completed = 0\n",
    "            total = len(future_to_info)\n",
    "\n",
    "            for future in as_completed(future_to_info):\n",
    "                date, cat = future_to_info[future]\n",
    "                completed += 1\n",
    "\n",
    "                try:\n",
    "                    results = future.result()\n",
    "                    if results:\n",
    "                        all_rows.extend(results)\n",
    "                    else:\n",
    "                        failed_requests.append((date, cat))\n",
    "\n",
    "                    # Progress bar process per 50\n",
    "                    if completed % 50 == 0:\n",
    "                        logger.info(f\"[{category}] Progress: {completed}/{total} ({completed/total*100:.1f}%)\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"[{category}] {date} - Future exception: {e}\")\n",
    "                    failed_requests.append((date, cat))\n",
    "\n",
    "        # retry failed request\n",
    "        if failed_requests:\n",
    "            logger.info(f\"[{category}] Retrying {len(failed_requests)} failed requests\")\n",
    "            retry_count = 0\n",
    "            for date, cat in failed_requests[:]:  # copy failed lists\n",
    "                time.sleep(random.uniform(3, 6))  # set retry timeout\n",
    "                retry_results = self.fetch_single_day(date, cat)\n",
    "                if retry_results:\n",
    "                    all_rows.extend(retry_results)\n",
    "                    failed_requests.remove((date, cat))\n",
    "                    retry_count += 1\n",
    "\n",
    "                # Retry progress\n",
    "                if retry_count % 10 == 0:\n",
    "                    logger.info(f\"[{category}] Retry progress: {retry_count}\")\n",
    "\n",
    "        category_elapsed = time.time() - category_start\n",
    "        logger.info(f\"[{category}] Completed in {category_elapsed:.2f}s, \"\n",
    "                   f\"got {len(all_rows)} articles, {len(failed_requests)} failed\")\n",
    "\n",
    "        return all_rows, failed_requests\n",
    "\n",
    "    def save_category_to_csv(self, rows, category, months):\n",
    "        \"\"\"saved sing. category\"\"\"\n",
    "        if not rows:\n",
    "            logger.warning(f\"[{category}] No data to save\")\n",
    "            return 0\n",
    "\n",
    "        # unique articles\n",
    "        unique_articles = {}\n",
    "        for date, article_id, url, cat in rows:\n",
    "            key = f\"{cat}_{article_id}\"\n",
    "            if key not in unique_articles:\n",
    "                unique_articles[key] = (date, article_id, url, cat)\n",
    "            else:\n",
    "                existing_date = unique_articles[key][0]\n",
    "                if date < existing_date:\n",
    "                    unique_articles[key] = (date, article_id, url, cat)\n",
    "\n",
    "        # sorted and write into file\n",
    "        sorted_articles = sorted(unique_articles.values())\n",
    "\n",
    "        month_range = f\"{months[0]}-{months[-1]}\"\n",
    "        filename = f\"appledaily_{category}_{month_range}.csv\"\n",
    "\n",
    "        with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"date\", \"article_id\", \"url\", \"category\"])\n",
    "            for row in sorted_articles:\n",
    "                writer.writerow(row)\n",
    "\n",
    "        logger.info(f\"[{category}] Saved {len(sorted_articles)} unique articles to {filename}\")\n",
    "        return len(sorted_articles), filename\n",
    "\n",
    "    def save_all_categories_to_csv(self, all_data, months):\n",
    "        \"\"\"save all categories to csv\"\"\"\n",
    "        if not all_data:\n",
    "            logger.warning(\"No data to save to combined file\")\n",
    "            return 0, \"\"\n",
    "\n",
    "        # combine all data and output to csv\n",
    "        unique_articles = {}\n",
    "        for rows in all_data.values():\n",
    "            for date, article_id, url, category in rows:\n",
    "                key = f\"{category}_{article_id}\"\n",
    "                if key not in unique_articles:\n",
    "                    unique_articles[key] = (date, article_id, url, category)\n",
    "                else:\n",
    "                    existing_date = unique_articles[key][0]\n",
    "                    if date < existing_date:\n",
    "                        unique_articles[key] = (date, article_id, url, category)\n",
    "\n",
    "        # sorted and write into file\n",
    "        sorted_articles = sorted(unique_articles.values())\n",
    "\n",
    "        month_range = f\"{months[0]}-{months[-1]}\"\n",
    "        filename = f\"appledaily_ALL_CATEGORIES_{month_range}.csv\"\n",
    "\n",
    "        with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"date\", \"article_id\", \"url\", \"category\"])\n",
    "            for row in sorted_articles:\n",
    "                writer.writerow(row)\n",
    "\n",
    "        logger.info(f\"Saved {len(sorted_articles)} unique articles (all categories) to {filename}\")\n",
    "        return len(sorted_articles), filename\n",
    "```\n",
    "\n",
    "```python\n",
    "def main():\n",
    "    # === set variables ===\n",
    "    months = [\"202201\", \"202202\", \"202203\", \"202204\", \"202205\"]\n",
    "    categories = [\n",
    "        \"local\",        # 地方新聞\n",
    "        \"realtime\",     # 即時新聞\n",
    "        \"entertainment\", # 娛樂新聞\n",
    "        \"sports\",       # 體育新聞\n",
    "        \"international\", # 國際新聞\n",
    "        \"finance\",      # 財經新聞\n",
    "        \"life\",         # 生活新聞\n",
    "        \"forum\"         # 論壇\n",
    "    ]\n",
    "\n",
    "    # scraper\n",
    "    scraper = MultiCategoryWaybackScraper(\n",
    "        max_workers=2,      # 2workers\n",
    "        max_retries=3,      # retry times\n",
    "        timeout=60          # timeout:60s\n",
    "    )\n",
    "\n",
    "    # start scrapping\n",
    "    overall_start = time.time()\n",
    "    all_category_data = {}\n",
    "    category_files = []\n",
    "\n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(f\"STARTING MULTI-CATEGORY SCRAPING\")\n",
    "    logger.info(f\"Months: {months}\")\n",
    "    logger.info(f\"Categories: {categories}\")\n",
    "    logger.info(\"=\" * 80)\n",
    "\n",
    "    for i, category in enumerate(categories, 1):\n",
    "        logger.info(f\"\\n{'='*20} CATEGORY {i}/{len(categories)}: {category.upper()} {'='*20}\")\n",
    "\n",
    "        category_rows, failed = scraper.fetch_category_data(months, category)\n",
    "        all_category_data[category] = category_rows\n",
    "\n",
    "        # saved sing. category\n",
    "        if category_rows:\n",
    "            count, filename = scraper.save_category_to_csv(category_rows, category, months)\n",
    "            category_files.append((category, count, filename))\n",
    "        else:\n",
    "            logger.warning(f\"[{category}] No data found!\")\n",
    "            category_files.append((category, 0, \"No file\"))\n",
    "\n",
    "        # sleep between both category\n",
    "        if i < len(categories):\n",
    "            rest_time = random.uniform(30, 60)\n",
    "            logger.info(f\"Resting {rest_time:.1f}s before next category...\")\n",
    "            time.sleep(rest_time)\n",
    "\n",
    "    # saved all news\n",
    "    total_count, combined_file = scraper.save_all_categories_to_csv(all_category_data, months)\n",
    "\n",
    "    # final report\n",
    "    total_time = time.time() - overall_start\n",
    "\n",
    "    logger.info(\"\\n\" + \"=\" * 80)\n",
    "    logger.info(\"FINAL REPORT\")\n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(f\"Total execution time: {total_time:.2f} seconds ({total_time/60:.1f} minutes)\")\n",
    "    logger.info(f\"Months processed: {', '.join(months)}\")\n",
    "    logger.info(f\"Categories processed: {len(categories)}\")\n",
    "\n",
    "    logger.info(\"\\nPer-category results:\")\n",
    "    total_articles = 0\n",
    "    for category, count, filename in category_files:\n",
    "        logger.info(f\"  {category:>15}: {count:>6} articles → {filename}\")\n",
    "        total_articles += count\n",
    "\n",
    "    logger.info(f\"\\nCombined results:\")\n",
    "    logger.info(f\"  Total raw articles: {total_articles}\")\n",
    "    logger.info(f\"  Unique articles: {total_count}\")\n",
    "    logger.info(f\"  Combined file: {combined_file}\")\n",
    "\n",
    "    logger.info(\"\\nSuccess rates by category:\")\n",
    "    for category in categories:\n",
    "        stats = scraper.stats[category]\n",
    "        if stats[\"success\"] + stats[\"failed\"] > 0:\n",
    "            success_rate = stats[\"success\"] / (stats[\"success\"] + stats[\"failed\"]) * 100\n",
    "            logger.info(f\"  {category:>15}: {success_rate:>5.1f}% \"\n",
    "                       f\"({stats['success']}/{stats['success'] + stats['failed']} requests, \"\n",
    "                       f\"{stats['total_items']} items)\")\n",
    "        else:\n",
    "            logger.info(f\"  {category:>15}: No requests made\")\n",
    "\n",
    "    # create SUMMARY file\n",
    "    summary_file = f\"SUMMARY_appledaily_{months[0]}-{months[-1]}.txt\"\n",
    "    with open(summary_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Apple Daily Archive Summary\\n\")\n",
    "        f.write(f\"Period: {months[0]} - {months[-1]}\\n\")\n",
    "        f.write(f\"Execution time: {total_time:.2f} seconds\\n\")\n",
    "        f.write(f\"Total unique articles: {total_count}\\n\\n\")\n",
    "        f.write(\"Files created:\\n\")\n",
    "        for category, count, filename in category_files:\n",
    "            if filename != \"No file\":\n",
    "                f.write(f\"  {filename}\\n\")\n",
    "        f.write(f\"  {combined_file}\\n\")\n",
    "\n",
    "    logger.info(f\"\\nSummary saved to: {summary_file}\")\n",
    "    logger.info(\"SCRAPING COMPLETED!\")\n",
    "```\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}